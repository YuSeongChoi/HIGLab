# Core ML AI Reference

> ì˜¨ë””ë°”ì´ìŠ¤ ë¨¸ì‹ ëŸ¬ë‹ ê°€ì´ë“œ. ì´ ë¬¸ì„œë¥¼ ì½ê³  Core ML ì½”ë“œë¥¼ ìƒì„±í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

## ê°œìš”

Core MLì€ í•™ìŠµëœ ML ëª¨ë¸ì„ ì•±ì—ì„œ ì‹¤í–‰í•˜ëŠ” í”„ë ˆì„ì›Œí¬ì…ë‹ˆë‹¤.
ì´ë¯¸ì§€ ë¶„ë¥˜, ê°ì²´ ê°ì§€, ìì—°ì–´ ì²˜ë¦¬ ë“± ë‹¤ì–‘í•œ ML ì‘ì—…ì„ ì˜¨ë””ë°”ì´ìŠ¤ì—ì„œ ìˆ˜í–‰í•©ë‹ˆë‹¤.

## í•„ìˆ˜ Import

```swift
import CoreML
import Vision  // ì´ë¯¸ì§€ ë¶„ì„ ì‹œ
```

## í•µì‹¬ êµ¬ì„±ìš”ì†Œ

### 1. ëª¨ë¸ ë¡œë“œ

```swift
// 1. ë²ˆë“¤ëœ ëª¨ë¸ (ì»´íŒŒì¼ëœ .mlmodelc)
let model = try? MyImageClassifier(configuration: MLModelConfiguration())

// 2. ë™ì  ë¡œë“œ (URLì—ì„œ)
let modelURL = Bundle.main.url(forResource: "MyModel", withExtension: "mlmodelc")!
let model = try MLModel(contentsOf: modelURL)

// 3. ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì»´íŒŒì¼
let sourceURL = Bundle.main.url(forResource: "MyModel", withExtension: "mlmodel")!
let compiledURL = try await MLModel.compileModel(at: sourceURL)
let model = try MLModel(contentsOf: compiledURL)
```

### 2. ëª¨ë¸ ì„¤ì •

```swift
let config = MLModelConfiguration()

// ì—°ì‚° ì¥ì¹˜ ì„ íƒ
config.computeUnits = .all        // CPU + GPU + Neural Engine
config.computeUnits = .cpuOnly    // CPUë§Œ
config.computeUnits = .cpuAndGPU  // Neural Engine ì œì™¸

// GPU í—ˆìš©
config.allowLowPrecisionAccumulationOnGPU = true

let model = try MyModel(configuration: config)
```

### 3. Vision + Core ML

```swift
func classifyImage(_ image: UIImage) async throws -> [(String, Float)] {
    guard let cgImage = image.cgImage else { throw ClassificationError.invalidImage }
    
    // Core ML ëª¨ë¸ì„ Vision ëª¨ë¸ë¡œ ë˜í•‘
    let model = try VNCoreMLModel(for: MobileNetV2().model)
    
    let request = VNCoreMLRequest(model: model)
    request.imageCropAndScaleOption = .centerCrop
    
    let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
    try handler.perform([request])
    
    guard let results = request.results as? [VNClassificationObservation] else {
        throw ClassificationError.noResults
    }
    
    return results.prefix(5).map { ($0.identifier, $0.confidence) }
}
```

## ì „ì²´ ì‘ë™ ì˜ˆì œ

### ì´ë¯¸ì§€ ë¶„ë¥˜ê¸°

```swift
import SwiftUI
import CoreML
import Vision
import PhotosUI

// MARK: - Classifier
@Observable
class ImageClassifier {
    var predictions: [(label: String, confidence: Float)] = []
    var isProcessing = false
    var error: Error?
    
    private var model: VNCoreMLModel?
    
    init() {
        setupModel()
    }
    
    private func setupModel() {
        do {
            // MobileNetV2 ëª¨ë¸ ì‚¬ìš© (Apple ì œê³µ)
            let config = MLModelConfiguration()
            config.computeUnits = .all
            let coreMLModel = try MobileNetV2(configuration: config).model
            model = try VNCoreMLModel(for: coreMLModel)
        } catch {
            self.error = error
        }
    }
    
    func classify(_ image: UIImage) async {
        guard let cgImage = image.cgImage, let model = model else { return }
        
        isProcessing = true
        defer { isProcessing = false }
        
        let request = VNCoreMLRequest(model: model)
        request.imageCropAndScaleOption = .centerCrop
        
        let handler = VNImageRequestHandler(cgImage: cgImage, options: [:])
        
        do {
            try handler.perform([request])
            
            if let results = request.results as? [VNClassificationObservation] {
                await MainActor.run {
                    predictions = results.prefix(5).map { 
                        (label: $0.identifier.components(separatedBy: ",").first ?? $0.identifier, 
                         confidence: $0.confidence) 
                    }
                }
            }
        } catch {
            await MainActor.run {
                self.error = error
            }
        }
    }
}

// MARK: - View
struct ImageClassifierView: View {
    @State private var classifier = ImageClassifier()
    @State private var selectedItem: PhotosPickerItem?
    @State private var selectedImage: UIImage?
    
    var body: some View {
        NavigationStack {
            VStack(spacing: 20) {
                // ì´ë¯¸ì§€ ì„ íƒ
                PhotosPicker(selection: $selectedItem, matching: .images) {
                    Group {
                        if let image = selectedImage {
                            Image(uiImage: image)
                                .resizable()
                                .scaledToFit()
                        } else {
                            ContentUnavailableView("ì´ë¯¸ì§€ ì„ íƒ", systemImage: "photo", description: Text("ë¶„ë¥˜í•  ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ì„¸ìš”"))
                        }
                    }
                    .frame(maxHeight: 300)
                }
                
                // ê²°ê³¼
                if classifier.isProcessing {
                    ProgressView("ë¶„ì„ ì¤‘...")
                } else if !classifier.predictions.isEmpty {
                    VStack(alignment: .leading, spacing: 12) {
                        Text("ë¶„ë¥˜ ê²°ê³¼")
                            .font(.headline)
                        
                        ForEach(classifier.predictions, id: \.label) { prediction in
                            HStack {
                                Text(prediction.label)
                                Spacer()
                                Text("\(Int(prediction.confidence * 100))%")
                                    .foregroundStyle(.secondary)
                            }
                            
                            ProgressView(value: prediction.confidence)
                                .tint(prediction.confidence > 0.5 ? .green : .orange)
                        }
                    }
                    .padding()
                    .background(.regularMaterial)
                    .clipShape(RoundedRectangle(cornerRadius: 12))
                }
                
                Spacer()
            }
            .padding()
            .navigationTitle("ì´ë¯¸ì§€ ë¶„ë¥˜")
            .onChange(of: selectedItem) { _, newItem in
                Task {
                    if let data = try? await newItem?.loadTransferable(type: Data.self),
                       let image = UIImage(data: data) {
                        selectedImage = image
                        await classifier.classify(image)
                    }
                }
            }
        }
    }
}
```

### í…ìŠ¤íŠ¸ ë¶„ë¥˜

```swift
import NaturalLanguage

@Observable
class SentimentAnalyzer {
    var sentiment: String = ""
    var confidence: Double = 0
    
    func analyze(_ text: String) {
        let tagger = NLTagger(tagSchemes: [.sentimentScore])
        tagger.string = text
        
        let (sentiment, _) = tagger.tag(at: text.startIndex, unit: .paragraph, scheme: .sentimentScore)
        
        if let sentimentScore = sentiment?.rawValue, let score = Double(sentimentScore) {
            self.confidence = abs(score)
            if score > 0.1 {
                self.sentiment = "ê¸ì •ì  ğŸ˜Š"
            } else if score < -0.1 {
                self.sentiment = "ë¶€ì •ì  ğŸ˜"
            } else {
                self.sentiment = "ì¤‘ë¦½ì  ğŸ˜"
            }
        }
    }
}
```

## ê³ ê¸‰ íŒ¨í„´

### 1. ì»¤ìŠ¤í…€ ëª¨ë¸ ì‚¬ìš©

```swift
// 1. Create MLë¡œ í•™ìŠµí•œ ëª¨ë¸
// 2. Xcode í”„ë¡œì íŠ¸ì— .mlmodel íŒŒì¼ ì¶”ê°€
// 3. ìë™ ìƒì„±ëœ í´ë˜ìŠ¤ ì‚¬ìš©

class CustomClassifier {
    let model: MyCustomModel
    
    init() throws {
        let config = MLModelConfiguration()
        model = try MyCustomModel(configuration: config)
    }
    
    func predict(input: MLMultiArray) throws -> MyCustomModelOutput {
        let input = MyCustomModelInput(features: input)
        return try model.prediction(input: input)
    }
}
```

### 2. ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ë¶„ë¥˜

```swift
import AVFoundation

class CameraClassifier: NSObject, AVCaptureVideoDataOutputSampleBufferDelegate {
    var onPrediction: ([(String, Float)]) -> Void = { _ in }
    
    private let model: VNCoreMLModel
    private let captureSession = AVCaptureSession()
    
    init() throws {
        let coreModel = try MobileNetV2(configuration: MLModelConfiguration()).model
        model = try VNCoreMLModel(for: coreModel)
        super.init()
        setupCamera()
    }
    
    private func setupCamera() {
        guard let device = AVCaptureDevice.default(for: .video),
              let input = try? AVCaptureDeviceInput(device: device) else { return }
        
        captureSession.addInput(input)
        
        let output = AVCaptureVideoDataOutput()
        output.setSampleBufferDelegate(self, queue: DispatchQueue(label: "ml.queue"))
        captureSession.addOutput(output)
    }
    
    func captureOutput(_ output: AVCaptureOutput, didOutput sampleBuffer: CMSampleBuffer, from connection: AVCaptureConnection) {
        guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
        
        let request = VNCoreMLRequest(model: model) { [weak self] request, _ in
            guard let results = request.results as? [VNClassificationObservation] else { return }
            let predictions = results.prefix(3).map { ($0.identifier, $0.confidence) }
            
            DispatchQueue.main.async {
                self?.onPrediction(predictions)
            }
        }
        
        let handler = VNImageRequestHandler(cvPixelBuffer: pixelBuffer, options: [:])
        try? handler.perform([request])
    }
}
```

### 3. ëª¨ë¸ ì—…ë°ì´íŠ¸ (On-Device Training)

```swift
// Updatable ëª¨ë¸ í•„ìš” (.mlmodelì—ì„œ ì„¤ì •)
func updateModel(with trainingData: MLBatchProvider) async throws {
    let modelURL = Bundle.main.url(forResource: "UpdatableModel", withExtension: "mlmodelc")!
    
    let updateTask = try MLUpdateTask(
        forModelAt: modelURL,
        trainingData: trainingData,
        configuration: nil,
        completionHandler: { context in
            // ì—…ë°ì´íŠ¸ëœ ëª¨ë¸ ì €ì¥
            let updatedModelURL = context.model.modelDescription.metadata[MLModelMetadataKey.creatorDefinedKey(key: "updatedModelURL")]
        }
    )
    
    updateTask.resume()
}
```

## ì£¼ì˜ì‚¬í•­

1. **ëª¨ë¸ í¬ê¸°**
   - ì•± ë²ˆë“¤ í¬ê¸°ì— ì˜í–¥
   - í° ëª¨ë¸ì€ On-Demand Resources ê³ ë ¤
   - ì–‘ìí™”ë¡œ í¬ê¸° ì¶•ì†Œ ê°€ëŠ¥

2. **ì„±ëŠ¥ ìµœì í™”**
   ```swift
   // Neural Engine ìš°ì„  ì‚¬ìš©
   config.computeUnits = .all
   
   // ì €ì „ë ¥ ëª¨ë“œì—ì„œ CPUë§Œ
   if ProcessInfo.processInfo.isLowPowerModeEnabled {
       config.computeUnits = .cpuOnly
   }
   ```

3. **ì…ë ¥ ì „ì²˜ë¦¬**
   - ëª¨ë¸ì´ ìš”êµ¬í•˜ëŠ” ì´ë¯¸ì§€ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
   - ì •ê·œí™” í•„ìš”í•œ ê²½ìš° ì§ì ‘ ì²˜ë¦¬
   - Vision ì‚¬ìš© ì‹œ ìë™ ì²˜ë¦¬ë¨

4. **ì—ëŸ¬ ì²˜ë¦¬**
   ```swift
   do {
       let prediction = try model.prediction(input: input)
   } catch MLModelError.generic {
       // ì¼ë°˜ ì˜¤ë¥˜
   } catch MLModelError.io {
       // ì…ì¶œë ¥ ì˜¤ë¥˜
   } catch {
       // ê¸°íƒ€ ì˜¤ë¥˜
   }
   ```
