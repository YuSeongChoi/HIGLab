# MLClassifier

CoreMLê³¼ Vision í”„ë ˆì„ì›Œí¬ë¥¼ ì‚¬ìš©í•œ ì´ë¯¸ì§€ ë¶„ë¥˜ ìƒ˜í”Œ ì•±ì…ë‹ˆë‹¤.

## ê°œìš”

ì´ í”„ë¡œì íŠ¸ëŠ” Appleì˜ CoreMLê³¼ Vision í”„ë ˆì„ì›Œí¬ë¥¼ í™œìš©í•˜ì—¬ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•˜ëŠ” ë°©ë²•ì„ ë³´ì—¬ì¤ë‹ˆë‹¤.

### ì£¼ìš” ê¸°ëŠ¥

- ğŸ“· **ì‚¬ì§„ ë¶„ë¥˜**: ì‚¬ì§„ ë¼ì´ë¸ŒëŸ¬ë¦¬ì—ì„œ ì´ë¯¸ì§€ë¥¼ ì„ íƒí•˜ì—¬ ë¶„ë¥˜
- ğŸ¥ **ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ë¶„ë¥˜**: ì¹´ë©”ë¼ í”¼ë“œë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ ë¶„ë¥˜
- ğŸ§  **ë‹¤ì¤‘ ëª¨ë¸ ì§€ì›**: MobileNetV2, ResNet50, SqueezeNet ëª¨ë¸ ì„ íƒ ê°€ëŠ¥
- ğŸ“Š **ì‹ ë¢°ë„ ì‹œê°í™”**: ë¶„ë¥˜ ê²°ê³¼ì˜ ì‹ ë¢°ë„ë¥¼ í”„ë¡œê·¸ë ˆìŠ¤ ë°”ë¡œ í‘œì‹œ

## í”„ë¡œì íŠ¸ êµ¬ì¡°

```
MLClassifier/
â”œâ”€â”€ Shared/                     # ê³µìœ  ì½”ë“œ
â”‚   â”œâ”€â”€ ClassificationResult.swift  # ë¶„ë¥˜ ê²°ê³¼ ëª¨ë¸
â”‚   â”œâ”€â”€ MLModelManager.swift        # ML ëª¨ë¸ ê´€ë¦¬
â”‚   â””â”€â”€ ImageClassifier.swift       # Vision + CoreML ë¶„ë¥˜ê¸°
â”‚
â”œâ”€â”€ MLClassifierApp/            # ì•± ì½”ë“œ
â”‚   â”œâ”€â”€ MLClassifierApp.swift       # @main ì•± ì§„ì…ì 
â”‚   â”œâ”€â”€ ContentView.swift           # ë©”ì¸ ë·° (íƒ­ ë·°)
â”‚   â”œâ”€â”€ PhotoClassifyView.swift     # ì‚¬ì§„ ë¶„ë¥˜ ë·°
â”‚   â”œâ”€â”€ CameraClassifyView.swift    # ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ë¶„ë¥˜ ë·°
â”‚   â””â”€â”€ ResultsView.swift           # ê²°ê³¼ í‘œì‹œ ì»´í¬ë„ŒíŠ¸
â”‚
â””â”€â”€ README.md
```

## í•µì‹¬ ê¸°ìˆ 

### VNCoreMLRequest

Vision í”„ë ˆì„ì›Œí¬ì˜ `VNCoreMLRequest`ë¥¼ ì‚¬ìš©í•˜ì—¬ CoreML ëª¨ë¸ë¡œ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:

```swift
// Vision ëª¨ë¸ ìƒì„±
let visionModel = try VNCoreMLModel(for: mlModel)

// ë¶„ë¥˜ ìš”ì²­ ìƒì„±
let request = VNCoreMLRequest(model: visionModel) { request, error in
    guard let observations = request.results as? [VNClassificationObservation] else {
        return
    }
    
    // ê²°ê³¼ ì²˜ë¦¬
    let results = observations.map { observation in
        ClassificationResult(
            label: observation.identifier,
            confidence: observation.confidence
        )
    }
}

// ì´ë¯¸ì§€ í•¸ë“¤ëŸ¬ë¡œ ìš”ì²­ ì‹¤í–‰
let handler = VNImageRequestHandler(cgImage: cgImage)
try handler.perform([request])
```

### ì‹¤ì‹œê°„ ì¹´ë©”ë¼ ë¶„ë¥˜

`AVCaptureVideoDataOutput`ì„ ì‚¬ìš©í•˜ì—¬ ì¹´ë©”ë¼ í”„ë ˆì„ì„ ìº¡ì²˜í•˜ê³ , ê° í”„ë ˆì„ì— ëŒ€í•´ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤:

```swift
// í”„ë ˆì„ ìº¡ì²˜ ë¸ë¦¬ê²Œì´íŠ¸
func captureOutput(_ output: AVCaptureOutput,
                   didOutput sampleBuffer: CMSampleBuffer,
                   from connection: AVCaptureConnection) {
    guard let pixelBuffer = CMSampleBufferGetImageBuffer(sampleBuffer) else { return }
    let ciImage = CIImage(cvPixelBuffer: pixelBuffer)
    
    // ë¶„ë¥˜ ìˆ˜í–‰
    Task {
        try await classifier.classify(ciImage: ciImage)
    }
}
```

## ML ëª¨ë¸ ì¶”ê°€

ì´ ì•±ì€ Appleì—ì„œ ì œê³µí•˜ëŠ” ì‚¬ì „ í•™ìŠµëœ ëª¨ë¸ì„ ì‚¬ìš©í•©ë‹ˆë‹¤:

1. [Apple ML Models](https://developer.apple.com/machine-learning/models/)ì—ì„œ ëª¨ë¸ ë‹¤ìš´ë¡œë“œ
2. `.mlmodel` íŒŒì¼ì„ Xcode í”„ë¡œì íŠ¸ì— ì¶”ê°€
3. Xcodeê°€ ìë™ìœ¼ë¡œ `.mlmodelc`ë¡œ ì»´íŒŒì¼

### ì§€ì› ëª¨ë¸

| ëª¨ë¸ | ì„¤ëª… | í¬ê¸° |
|-----|-----|-----|
| MobileNetV2 | ëª¨ë°”ì¼ ìµœì í™”, ë¹ ë¥¸ ì¶”ë¡  | ~14MB |
| ResNet50 | ë†’ì€ ì •í™•ë„ | ~98MB |
| SqueezeNet | ê²½ëŸ‰í™” ëª¨ë¸ | ~5MB |

## í•„ìš” ê¶Œí•œ

ì•±ì´ ì •ìƒì ìœ¼ë¡œ ë™ì‘í•˜ë ¤ë©´ ë‹¤ìŒ ê¶Œí•œì´ í•„ìš”í•©ë‹ˆë‹¤:

### Info.plist

```xml
<!-- ì‚¬ì§„ ë¼ì´ë¸ŒëŸ¬ë¦¬ ì ‘ê·¼ -->
<key>NSPhotoLibraryUsageDescription</key>
<string>ì‚¬ì§„ì„ ì„ íƒí•˜ì—¬ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìˆ˜í–‰í•©ë‹ˆë‹¤.</string>

<!-- ì¹´ë©”ë¼ ì ‘ê·¼ -->
<key>NSCameraUsageDescription</key>
<string>ì‹¤ì‹œê°„ ì´ë¯¸ì§€ ë¶„ë¥˜ë¥¼ ìœ„í•´ ì¹´ë©”ë¼ì— ì ‘ê·¼í•©ë‹ˆë‹¤.</string>
```

## í”Œë«í¼ ì§€ì›

- iOS 17.0+
- macOS 14.0+ (ì¹´ë©”ë¼ ê¸°ëŠ¥ ì œí•œì )

## ì°¸ê³  ìë£Œ

- [Vision Framework](https://developer.apple.com/documentation/vision)
- [Core ML](https://developer.apple.com/documentation/coreml)
- [VNCoreMLRequest](https://developer.apple.com/documentation/vision/vncoremlrequest)
- [Classifying Images with Vision and Core ML](https://developer.apple.com/documentation/vision/classifying_images_with_vision_and_core_ml)
