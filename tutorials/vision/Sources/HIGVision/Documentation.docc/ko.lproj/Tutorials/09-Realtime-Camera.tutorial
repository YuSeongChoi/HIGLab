@Tutorial(time: 25) {
    @Intro(title: "실시간 카메라 처리") {
        Vision과 AVFoundation을 결합해 
        실시간 카메라 스트림에서 비전 분석을 수행합니다.
    }
    
    @Section(title: "AVFoundation 카메라 설정") {
        @ContentAndMedia {
            실시간 Vision 처리를 위해 
            AVCaptureSession을 설정합니다.
            
            카메라 입력과 비디오 출력을 구성해
            프레임별로 Vision 분석을 실행합니다.
        }
        
        @Steps {
            @Step {
                **카메라 권한 요청**
                
                카메라 사용을 위해 먼저 권한을 요청합니다.
                Info.plist에 NSCameraUsageDescription 키도 필요합니다.
                
                @Code(name: "CameraManager.swift", file: "09-camera-permission.swift")
            }
            
            @Step {
                **AVCaptureSession 구성**
                
                카메라 입력과 비디오 데이터 출력을 설정합니다.
                
                @Code(name: "CameraManager.swift", file: "09-capture-session.swift")
            }
            
            @Step {
                **비디오 데이터 출력 설정**
                
                AVCaptureVideoDataOutput을 구성해
                각 프레임을 받을 델리게이트를 설정합니다.
                
                @Code(name: "CameraManager.swift", file: "09-video-output.swift")
            }
            
            @Step {
                **프레임 처리 큐**
                
                Vision 분석을 위한 별도의 직렬 큐를 생성합니다.
                메인 스레드 블로킹을 방지합니다.
                
                @Code(name: "CameraManager.swift", file: "09-processing-queue.swift")
            }
        }
    }
    
    @Section(title: "프레임별 Vision 분석") {
        @ContentAndMedia {
            AVCaptureVideoDataOutputSampleBufferDelegate를 구현해
            각 프레임에서 Vision 분석을 실행합니다.
        }
        
        @Steps {
            @Step {
                **델리게이트 구현**
                
                captureOutput(_:didOutput:from:) 메서드에서 
                샘플 버퍼를 받습니다.
                
                @Code(name: "CameraManager.swift", file: "09-delegate-method.swift")
            }
            
            @Step {
                **CMSampleBuffer에서 Vision 분석**
                
                샘플 버퍼로 VNImageRequestHandler를 생성하고 
                Request를 실행합니다.
                
                @Code(name: "CameraManager.swift", file: "09-sample-buffer-vision.swift")
            }
            
            @Step {
                **프레임 드롭 처리**
                
                Vision 처리가 프레임 속도를 따라가지 못할 때
                프레임을 건너뛰는 전략을 구현합니다.
                
                @Code(name: "CameraManager.swift", file: "09-frame-drop.swift")
            }
            
            @Step {
                **결과 메인 스레드 전달**
                
                Vision 결과를 메인 스레드로 전달해 UI를 업데이트합니다.
                
                @Code(name: "CameraManager.swift", file: "09-main-thread.swift")
            }
        }
    }
    
    @Section(title: "카메라 프리뷰 UI") {
        @ContentAndMedia {
            SwiftUI에서 카메라 프리뷰를 표시하고
            Vision 결과 오버레이를 렌더링합니다.
        }
        
        @Steps {
            @Step {
                **UIViewRepresentable 카메라 뷰**
                
                AVCaptureVideoPreviewLayer를 감싸는 
                UIViewRepresentable을 구현합니다.
                
                @Code(name: "CameraPreviewView.swift", file: "09-preview-representable.swift")
            }
            
            @Step {
                **결과 오버레이 레이어**
                
                Vision 결과를 그리는 CAShapeLayer를 추가합니다.
                
                @Code(name: "CameraPreviewView.swift", file: "09-overlay-layer.swift")
            }
            
            @Step {
                **좌표 변환**
                
                Vision 좌표를 프리뷰 레이어 좌표로 변환합니다.
                videoPreviewLayer의 메서드를 활용합니다.
                
                @Code(name: "CameraPreviewView.swift", file: "09-coord-transform.swift")
            }
        }
    }
    
    @Section(title: "성능 최적화") {
        @ContentAndMedia {
            실시간 처리를 위한 성능 최적화 기법을 적용합니다.
        }
        
        @Steps {
            @Step {
                **해상도 조절**
                
                sessionPreset을 조절해 처리할 프레임 해상도를 낮춥니다.
                
                @Code(name: "CameraManager.swift", file: "09-resolution-preset.swift")
            }
            
            @Step {
                **Request 재사용**
                
                매 프레임마다 새 Request를 생성하지 않고 재사용합니다.
                
                @Code(name: "CameraManager.swift", file: "09-request-reuse.swift")
            }
            
            @Step {
                **실시간 모드 설정**
                
                가능한 경우 Request의 실시간 모드를 활성화합니다.
                (.fast 레벨 사용 등)
                
                @Code(name: "CameraManager.swift", file: "09-realtime-mode.swift")
            }
            
            @Step {
                **완성된 실시간 스캐너**
                
                모든 요소를 통합한 실시간 문서 스캐너를 완성합니다.
                
                @Code(name: "RealtimeScannerView.swift", file: "09-complete-scanner.swift")
            }
        }
    }
    
    @Assessments {
        @MultipleChoice {
            실시간 비디오 프레임을 받기 위해 구현해야 하는 프로토콜은?
            
            @Choice(isCorrect: true) {
                AVCaptureVideoDataOutputSampleBufferDelegate
                
                @Justification(reaction: "정답!") {
                    이 델리게이트의 captureOutput 메서드에서
                    각 비디오 프레임을 CMSampleBuffer로 받습니다.
                }
            }
            
            @Choice(isCorrect: false) {
                AVCapturePhotoCaptureDelegate
                
                @Justification(reaction: "아쉽네요") {
                    PhotoCaptureDelegate는 사진 촬영용입니다.
                    연속 비디오 프레임은 VideoDataOutputSampleBufferDelegate입니다.
                }
            }
            
            @Choice(isCorrect: false) {
                AVCaptureSessionDelegate
                
                @Justification(reaction: "아쉽네요") {
                    그런 프로토콜은 없습니다.
                    비디오 프레임은 AVCaptureVideoDataOutputSampleBufferDelegate로 받습니다.
                }
            }
        }
        
        @MultipleChoice {
            Vision 처리로 인한 메인 스레드 블로킹을 방지하려면?
            
            @Choice(isCorrect: true) {
                별도의 직렬 큐에서 Vision 분석을 실행한다
                
                @Justification(reaction: "정확합니다!") {
                    Vision 처리는 비동기로, 백그라운드 큐에서 실행해야
                    UI가 멈추지 않습니다.
                }
            }
            
            @Choice(isCorrect: false) {
                메인 스레드에서 모든 처리를 한다
                
                @Justification(reaction: "아쉽네요") {
                    메인 스레드에서 무거운 처리를 하면
                    UI가 멈춥니다.
                }
            }
            
            @Choice(isCorrect: false) {
                처리 속도를 신경쓰지 않는다
                
                @Justification(reaction: "아쉽네요") {
                    실시간 앱에서 성능은 매우 중요합니다.
                    별도 큐에서 비동기 처리해야 합니다.
                }
            }
        }
        
        @MultipleChoice {
            Vision 좌표를 카메라 프리뷰 좌표로 변환할 때 사용하는 메서드는?
            
            @Choice(isCorrect: true) {
                videoPreviewLayer.layerPointConverted(fromCaptureDevicePoint:)
                
                @Justification(reaction: "정답!") {
                    AVCaptureVideoPreviewLayer는 좌표 변환 메서드를 제공합니다.
                    Vision의 정규화 좌표를 프리뷰 레이어 좌표로 변환합니다.
                }
            }
            
            @Choice(isCorrect: false) {
                CGAffineTransform
                
                @Justification(reaction: "아쉽네요") {
                    직접 변환도 가능하지만, videoPreviewLayer의
                    변환 메서드를 사용하는 것이 더 정확합니다.
                }
            }
            
            @Choice(isCorrect: false) {
                변환이 필요 없다
                
                @Justification(reaction: "아쉽네요") {
                    Vision은 정규화 좌표(0~1)를 사용하므로
                    화면 좌표로 변환이 필요합니다.
                }
            }
        }
    }
}
